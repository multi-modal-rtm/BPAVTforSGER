# BPAVTforSGER: Benchmarking Pre-trained Audio-Visual Transformers for Small-Group Engagement Recognition

This repository benchmarks pre-trained and scratch-built audio-visual transformer models on small-group engagement datasets like **OUC-CGE**, **AMI**, and **CMOSE**.

---

## ğŸ“ Project Structure

```
project-root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/               # All model architectures (scratch and pretrained)
â”‚   â”œâ”€â”€ data/                 # Dataloader and preprocessing scripts
â”‚   â”œâ”€â”€ train/                # Training and evaluation modules
â”‚   â””â”€â”€ config.yaml           # Model, dataset, and training configuration
â”œâ”€â”€ data/                     # Raw data and pre-extracted features (not tracked)
â”œâ”€â”€ checkpoints/              # Saved model checkpoints (not tracked)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Supported Models

You can switch models by setting the `model:` field in `config.yaml`:

### ğŸ› ï¸ Scratch Models (shared backbone: ResNet + Wav2Vec2)
- `MBTScratch`
- `VATTScratch`
- `AVAdapterScratch`
- `CrossModalAdapterScratch`

### ğŸ§  Pretrained Models (ViT + Wav2Vec2)
- `VATTPretrained`
- `AVAdapterPretrained`
- `CrossModalAdapter`

---

## ğŸš€ How to Run

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

2. Organize your data under `data/OUC-CGE/` or similar.

3. Modify `src/config.yaml` to select your model and dataset path:
   ```yaml
   model: "AVAdapterPretrained"
   ```

4. Run training + evaluation:
   ```bash
   python src/train/launch.py
   ```

---

## ğŸ“Š Evaluation Metrics
This framework supports:
- Accuracy / Balanced Accuracy
- F1 Score
- AUC-ROC
- mAP (mean Average Precision)
- CCC (Concordance Correlation Coefficient)
