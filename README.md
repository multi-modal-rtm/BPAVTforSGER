
# BPAVTforSGER: Benchmarking Pre-trained Audio-Visual Transformers for Small-Group Engagement Recognition

This repository implements a unified pipeline for benchmarking A/V transformers on three engagement datasets (DAiSEE, OUC-CGE, AMI). It supports four models (MBT, VATT, AVT, Video-Swin) with pretrained Kinetics-400 weights when available.

---

## ğŸ“ Project Structure


engagement\_recognition/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ manifests/           # CSV manifests for each dataset
â”‚   â””â”€â”€ raw/                 # raw video/audio files
â”œâ”€â”€ engagement\_recognition/  # core package
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ preprocess.py        # video/audio featurizers & transforms
â”‚   â”œâ”€â”€ dataloader.py        # Dataset & DataLoader factories
â”‚   â”œâ”€â”€ models.py            # model definitions & weight loading
â”‚   â”œâ”€â”€ train.py             # training loop & checkpointing
â”‚   â”œâ”€â”€ test.py              # evaluation, metrics, checkpoint loading
â”‚   â”œâ”€â”€ utils.py             # checkpointing, meters, mAP
â”‚   â””â”€â”€ callbacks.py         # (optional) schedulers / early stopping
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download\_data.sh     # helper to download raw datasets
â”‚   â””â”€â”€ extract\_features.py  # offline feature extraction
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ logs/                # TensorBoard logs
â”‚   â””â”€â”€ checkpoints/         # model checkpoints
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“¦ Installation

```bash
# Clone repo
git clone https://github.com/yourusername/engagement_recognition.git
cd engagement_recognition

# Create env & install
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
````

**`requirements.txt`** should include, at minimum:

```
torch
torchvision
torchaudio
omegaconf
numpy
pandas
scikit-learn
scenic       # for MBT
huggingface-hub
swin-transformer
# plus your VATT/AVT repos
```

---

## ğŸ“¥ Download & Prepare Data

Edit `scripts/download_data.sh` with correct URLs and run:

```bash
bash scripts/download_data.sh
```

This should:

1. Download and unzip **DAiSEE**, **OUC-CGE**, and **AMI** into `data/raw/`.
2. Generate CSV manifests in `data/manifests/` containing:

   * `video_path`
   * `audio_path`
   * `label` (mapped 0â€“3)
   * `split` (train/val/test)

---

## âš™ï¸ Configuration

All hyperparameters and paths live in `configs/default.yaml`.
Adjust:

* `data.roots` â†’ where your raw folders live
* `model.name` â†’ one of `mbt,vatt,avt,swin`
* `training.epochs, lr, fp16, resume_from`
* `logging.log_dir, ckpt_dir, log_interval`

---

## ğŸš€ Training

```bash
# From repo root
python -m engagement_recognition.train
```

* Checkpoints (`last.pth`, `best.pth`) appear under `outputs/checkpoints/`.
* TensorBoard logs under `outputs/logs/` â†’ `tensorboard --logdir outputs/logs`

---

## ğŸ” Testing & Evaluation

```bash
python -m engagement_recognition.test
```

This prints:

* Classification Report
* Confusion Matrix
* **mAP** and per-class AP

---

## ğŸ—„ Offline Feature Extraction

If youâ€™d like to precompute and cache embeddings:

```bash
python scripts/extract_features.py \
  --model mbt \
  --datasets daisee ouc_cge ami \
  --out_dir data/features
```

This script should:

1. Load each video/audio clip.
2. Run through `preprocess.extract_video` / `extract_audio`.
3. Forward through frozen backbone.
4. Save per-clip `.npy` feature files to `data/features/{dataset}/{model}/`.

---

## ğŸ”§ Callbacks & Extensions

* **callbacks.py** is a placeholder for learning-rate schedulers, early stopping, etc.
* Feel free to integrate PyTorch Lightning or Hydra for more flexibility.

---

## ğŸ“– Citation

If you use this code in publications, please cite:

> â€œBenchmarking Pre-trained Audio-Visual Transformers for Small-Group Engagement Recognition,â€ *BMVC 2025*.


